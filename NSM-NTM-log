/home/lmk/.conda/envs/keras/bin/python2.7 "/media/lmk/0101d15f-da66-48f5-a1f6-85b33d22f422/mhb/Deeplearning (copy)/RES/novel.py"
Using TensorFlow backend.
/home/lmk/.conda/envs/keras/lib/python2.7/site-packages/matplotlib/__init__.py:1405: UserWarning: 
This call to matplotlib.use() has no effect because the backend has already
been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,
or matplotlib.backends is imported for the first time.

  warnings.warn(_use_error_msg)
((960000, 128, 2), (240000, 10), (240000, 128, 2), (240000, 10))
//------------------------------------------------------------------------------------------------------------------------------------NTM
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
Input (InputLayer)              (None, 128, 2)       0                                            
__________________________________________________________________________________________________
res_stack1_a (Conv1D)           (None, 128, 16)      48          Input[0][0]                      
__________________________________________________________________________________________________
bn_stack1_a (BatchNormalization (None, 128, 16)      64          res_stack1_a[0][0]               
__________________________________________________________________________________________________
res_stack1_blockb_u1 (Conv1D)   (None, 128, 16)      2064        bn_stack1_a[0][0]                
__________________________________________________________________________________________________
bn_stack1_blockb_u1 (BatchNorma (None, 128, 16)      64          res_stack1_blockb_u1[0][0]       
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 128, 16)      0           bn_stack1_blockb_u1[0][0]        
__________________________________________________________________________________________________
res_stack1_blockb_u2 (Conv1D)   (None, 128, 16)      272         activation_1[0][0]               
__________________________________________________________________________________________________
bn_stack1_blockb_u2 (BatchNorma (None, 128, 16)      64          res_stack1_blockb_u2[0][0]       
__________________________________________________________________________________________________
add_1 (Add)                     (None, 128, 16)      0           bn_stack1_blockb_u2[0][0]        
                                                                 bn_stack1_a[0][0]                
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 128, 16)      0           add_1[0][0]                      
__________________________________________________________________________________________________
res_stack1_blockc_u1 (Conv1D)   (None, 128, 16)      2064        activation_2[0][0]               
__________________________________________________________________________________________________
bn_stack1_blockc_u1 (BatchNorma (None, 128, 16)      64          res_stack1_blockc_u1[0][0]       
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 128, 16)      0           bn_stack1_blockc_u1[0][0]        
__________________________________________________________________________________________________
res_stack1_blockc_u2 (Conv1D)   (None, 128, 16)      272         activation_3[0][0]               
__________________________________________________________________________________________________
bn_stack1_blockc_u2 (BatchNorma (None, 128, 16)      64          res_stack1_blockc_u2[0][0]       
__________________________________________________________________________________________________
add_2 (Add)                     (None, 128, 16)      0           bn_stack1_blockc_u2[0][0]        
                                                                 activation_2[0][0]               
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 128, 16)      0           add_2[0][0]                      
__________________________________________________________________________________________________
mp_stack1 (MaxPooling1D)        (None, 128, 16)      0           activation_4[0][0]               
__________________________________________________________________________________________________
res_stack2_a (Conv1D)           (None, 128, 32)      544         mp_stack1[0][0]                  
__________________________________________________________________________________________________
bn_stack2_a (BatchNormalization (None, 128, 32)      128         res_stack2_a[0][0]               
__________________________________________________________________________________________________
res_stack2_blockb_u1 (Conv1D)   (None, 128, 32)      8224        bn_stack2_a[0][0]                
__________________________________________________________________________________________________
bn_stack2_blockb_u1 (BatchNorma (None, 128, 32)      128         res_stack2_blockb_u1[0][0]       
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 128, 32)      0           bn_stack2_blockb_u1[0][0]        
__________________________________________________________________________________________________
res_stack2_blockb_u2 (Conv1D)   (None, 128, 32)      1056        activation_5[0][0]               
__________________________________________________________________________________________________
bn_stack2_blockb_u2 (BatchNorma (None, 128, 32)      128         res_stack2_blockb_u2[0][0]       
__________________________________________________________________________________________________
add_3 (Add)                     (None, 128, 32)      0           bn_stack2_blockb_u2[0][0]        
                                                                 bn_stack2_a[0][0]                
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 128, 32)      0           add_3[0][0]                      
__________________________________________________________________________________________________
res_stack2_blockc_u1 (Conv1D)   (None, 128, 32)      8224        activation_6[0][0]               
__________________________________________________________________________________________________
bn_stack2_blockc_u1 (BatchNorma (None, 128, 32)      128         res_stack2_blockc_u1[0][0]       
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 128, 32)      0           bn_stack2_blockc_u1[0][0]        
__________________________________________________________________________________________________
res_stack2_blockc_u2 (Conv1D)   (None, 128, 32)      1056        activation_7[0][0]               
__________________________________________________________________________________________________
bn_stack2_blockc_u2 (BatchNorma (None, 128, 32)      128         res_stack2_blockc_u2[0][0]       
__________________________________________________________________________________________________
add_4 (Add)                     (None, 128, 32)      0           bn_stack2_blockc_u2[0][0]        
                                                                 activation_6[0][0]               
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 128, 32)      0           add_4[0][0]                      
__________________________________________________________________________________________________
mp_stack2 (MaxPooling1D)        (None, 128, 32)      0           activation_8[0][0]               
__________________________________________________________________________________________________
res_stack3_a (Conv1D)           (None, 128, 48)      1584        mp_stack2[0][0]                  
__________________________________________________________________________________________________
bn_stack3_a (BatchNormalization (None, 128, 48)      192         res_stack3_a[0][0]               
__________________________________________________________________________________________________
res_stack3_blockb_u1 (Conv1D)   (None, 128, 48)      18480       bn_stack3_a[0][0]                
__________________________________________________________________________________________________
bn_stack3_blockb_u1 (BatchNorma (None, 128, 48)      192         res_stack3_blockb_u1[0][0]       
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 128, 48)      0           bn_stack3_blockb_u1[0][0]        
__________________________________________________________________________________________________
res_stack3_blockb_u2 (Conv1D)   (None, 128, 48)      2352        activation_9[0][0]               
__________________________________________________________________________________________________
bn_stack3_blockb_u2 (BatchNorma (None, 128, 48)      192         res_stack3_blockb_u2[0][0]       
__________________________________________________________________________________________________
add_5 (Add)                     (None, 128, 48)      0           bn_stack3_blockb_u2[0][0]        
                                                                 bn_stack3_a[0][0]                
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 128, 48)      0           add_5[0][0]                      
__________________________________________________________________________________________________
res_stack3_blockc_u1 (Conv1D)   (None, 128, 48)      18480       activation_10[0][0]              
__________________________________________________________________________________________________
bn_stack3_blockc_u1 (BatchNorma (None, 128, 48)      192         res_stack3_blockc_u1[0][0]       
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 128, 48)      0           bn_stack3_blockc_u1[0][0]        
__________________________________________________________________________________________________
res_stack3_blockc_u2 (Conv1D)   (None, 128, 48)      2352        activation_11[0][0]              
__________________________________________________________________________________________________
bn_stack3_blockc_u2 (BatchNorma (None, 128, 48)      192         res_stack3_blockc_u2[0][0]       
__________________________________________________________________________________________________
add_6 (Add)                     (None, 128, 48)      0           bn_stack3_blockc_u2[0][0]        
                                                                 activation_10[0][0]              
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 128, 48)      0           add_6[0][0]                      
__________________________________________________________________________________________________
mp_stack3 (MaxPooling1D)        (None, 128, 48)      0           activation_12[0][0]              
__________________________________________________________________________________________________
res_stack4_a (Conv1D)           (None, 128, 64)      3136        mp_stack3[0][0]                  
__________________________________________________________________________________________________
bn_stack4_a (BatchNormalization (None, 128, 64)      256         res_stack4_a[0][0]               
__________________________________________________________________________________________________
res_stack4_blockb_u1 (Conv1D)   (None, 128, 64)      32832       bn_stack4_a[0][0]                
__________________________________________________________________________________________________
bn_stack4_blockb_u1 (BatchNorma (None, 128, 64)      256         res_stack4_blockb_u1[0][0]       
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 128, 64)      0           bn_stack4_blockb_u1[0][0]        
__________________________________________________________________________________________________
res_stack4_blockb_u2 (Conv1D)   (None, 128, 64)      4160        activation_13[0][0]              
__________________________________________________________________________________________________
bn_stack4_blockb_u2 (BatchNorma (None, 128, 64)      256         res_stack4_blockb_u2[0][0]       
__________________________________________________________________________________________________
add_7 (Add)                     (None, 128, 64)      0           bn_stack4_blockb_u2[0][0]        
                                                                 bn_stack4_a[0][0]                
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 128, 64)      0           add_7[0][0]                      
__________________________________________________________________________________________________
res_stack4_blockc_u1 (Conv1D)   (None, 128, 64)      32832       activation_14[0][0]              
__________________________________________________________________________________________________
bn_stack4_blockc_u1 (BatchNorma (None, 128, 64)      256         res_stack4_blockc_u1[0][0]       
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 128, 64)      0           bn_stack4_blockc_u1[0][0]        
__________________________________________________________________________________________________
res_stack4_blockc_u2 (Conv1D)   (None, 128, 64)      4160        activation_15[0][0]              
__________________________________________________________________________________________________
bn_stack4_blockc_u2 (BatchNorma (None, 128, 64)      256         res_stack4_blockc_u2[0][0]       
__________________________________________________________________________________________________
add_8 (Add)                     (None, 128, 64)      0           bn_stack4_blockc_u2[0][0]        
                                                                 activation_14[0][0]              
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 128, 64)      0           add_8[0][0]                      
__________________________________________________________________________________________________
mp_stack4 (MaxPooling1D)        (None, 128, 64)      0           activation_16[0][0]              
__________________________________________________________________________________________________
res_stack5_a (Conv1D)           (None, 128, 80)      5200        mp_stack4[0][0]                  
__________________________________________________________________________________________________
bn_stack5_a (BatchNormalization (None, 128, 80)      320         res_stack5_a[0][0]               
__________________________________________________________________________________________________
res_stack5_blockb_u1 (Conv1D)   (None, 128, 80)      51280       bn_stack5_a[0][0]                
__________________________________________________________________________________________________
bn_stack5_blockb_u1 (BatchNorma (None, 128, 80)      320         res_stack5_blockb_u1[0][0]       
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 128, 80)      0           bn_stack5_blockb_u1[0][0]        
__________________________________________________________________________________________________
res_stack5_blockb_u2 (Conv1D)   (None, 128, 80)      6480        activation_17[0][0]              
__________________________________________________________________________________________________
bn_stack5_blockb_u2 (BatchNorma (None, 128, 80)      320         res_stack5_blockb_u2[0][0]       
__________________________________________________________________________________________________
add_9 (Add)                     (None, 128, 80)      0           bn_stack5_blockb_u2[0][0]        
                                                                 bn_stack5_a[0][0]                
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 128, 80)      0           add_9[0][0]                      
__________________________________________________________________________________________________
res_stack5_blockc_u1 (Conv1D)   (None, 128, 80)      51280       activation_18[0][0]              
__________________________________________________________________________________________________
bn_stack5_blockc_u1 (BatchNorma (None, 128, 80)      320         res_stack5_blockc_u1[0][0]       
__________________________________________________________________________________________________
activation_19 (Activation)      (None, 128, 80)      0           bn_stack5_blockc_u1[0][0]        
__________________________________________________________________________________________________
res_stack5_blockc_u2 (Conv1D)   (None, 128, 80)      6480        activation_19[0][0]              
__________________________________________________________________________________________________
bn_stack5_blockc_u2 (BatchNorma (None, 128, 80)      320         res_stack5_blockc_u2[0][0]       
__________________________________________________________________________________________________
add_10 (Add)                    (None, 128, 80)      0           bn_stack5_blockc_u2[0][0]        
                                                                 activation_18[0][0]              
__________________________________________________________________________________________________
activation_20 (Activation)      (None, 128, 80)      0           add_10[0][0]                     
__________________________________________________________________________________________________
mp_stack5 (MaxPooling1D)        (None, 128, 80)      0           activation_20[0][0]              
__________________________________________________________________________________________________
cu_dnnlstm_1 (CuDNNLSTM)        (None, 50)           26400       mp_stack5[0][0]                  
__________________________________________________________________________________________________
dense1 (Dense)                  (None, 128)          6528        cu_dnnlstm_1[0][0]               
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 128)          0           dense1[0][0]                     
__________________________________________________________________________________________________
dense3 (Dense)                  (None, 10)           1290        dropout_1[0][0]                  
__________________________________________________________________________________________________
T (Lambda)                      (None, 10)           0           dense3[0][0]                     
__________________________________________________________________________________________________
activation_21 (Activation)      (None, 10)           0           T[0][0]                          
==================================================================================================
Total params: 303,930
Trainable params: 301,530
Non-trainable params: 2,400
__________________________________________________________________________________________________
2019-12-05 12:00:03.254496: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-12-05 12:00:03.553198: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: 
name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335
pciBusID: 0000:04:00.0
totalMemory: 7.92GiB freeMemory: 7.80GiB
2019-12-05 12:00:03.757717: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 1 with properties: 
name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335
pciBusID: 0000:84:00.0
totalMemory: 7.92GiB freeMemory: 7.49GiB
2019-12-05 12:00:03.757798: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0, 1
2019-12-05 12:00:04.353030: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-12-05 12:00:04.353071: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 1 
2019-12-05 12:00:04.353078: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N N 
2019-12-05 12:00:04.353081: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 1:   N N 
2019-12-05 12:00:04.353472: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7540 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:04:00.0, compute capability: 6.1)
2019-12-05 12:00:04.443585: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 7233 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080, pci bus id: 0000:84:00.0, compute capability: 6.1)
('ResNetLSTMLikeModel:', [0.91491014270782467, 0.63785416663487748])
//------------------------------------------------------------------------------------------------------------------------------------NSM trained in the usual way
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
Input (InputLayer)              (None, 128, 2)       0                                            
__________________________________________________________________________________________________
res_stack1_a (Conv1D)           (None, 128, 16)      48          Input[0][0]                      
__________________________________________________________________________________________________
bn_stack1_a (BatchNormalization (None, 128, 16)      64          res_stack1_a[0][0]               
__________________________________________________________________________________________________
res_stack1_blockb_u1 (Conv1D)   (None, 128, 16)      2064        bn_stack1_a[0][0]                
__________________________________________________________________________________________________
bn_stack1_blockb_u1 (BatchNorma (None, 128, 16)      64          res_stack1_blockb_u1[0][0]       
__________________________________________________________________________________________________
activation_22 (Activation)      (None, 128, 16)      0           bn_stack1_blockb_u1[0][0]        
__________________________________________________________________________________________________
res_stack1_blockb_u2 (Conv1D)   (None, 128, 16)      272         activation_22[0][0]              
__________________________________________________________________________________________________
bn_stack1_blockb_u2 (BatchNorma (None, 128, 16)      64          res_stack1_blockb_u2[0][0]       
__________________________________________________________________________________________________
add_11 (Add)                    (None, 128, 16)      0           bn_stack1_blockb_u2[0][0]        
                                                                 bn_stack1_a[0][0]                
__________________________________________________________________________________________________
activation_23 (Activation)      (None, 128, 16)      0           add_11[0][0]                     
__________________________________________________________________________________________________
res_stack1_blockc_u1 (Conv1D)   (None, 128, 16)      2064        activation_23[0][0]              
__________________________________________________________________________________________________
bn_stack1_blockc_u1 (BatchNorma (None, 128, 16)      64          res_stack1_blockc_u1[0][0]       
__________________________________________________________________________________________________
activation_24 (Activation)      (None, 128, 16)      0           bn_stack1_blockc_u1[0][0]        
__________________________________________________________________________________________________
res_stack1_blockc_u2 (Conv1D)   (None, 128, 16)      272         activation_24[0][0]              
__________________________________________________________________________________________________
bn_stack1_blockc_u2 (BatchNorma (None, 128, 16)      64          res_stack1_blockc_u2[0][0]       
__________________________________________________________________________________________________
add_12 (Add)                    (None, 128, 16)      0           bn_stack1_blockc_u2[0][0]        
                                                                 activation_23[0][0]              
__________________________________________________________________________________________________
activation_25 (Activation)      (None, 128, 16)      0           add_12[0][0]                     
__________________________________________________________________________________________________
mp_stack1 (MaxPooling1D)        (None, 128, 16)      0           activation_25[0][0]              
__________________________________________________________________________________________________
res_stack2_a (Conv1D)           (None, 128, 32)      544         mp_stack1[0][0]                  
__________________________________________________________________________________________________
bn_stack2_a (BatchNormalization (None, 128, 32)      128         res_stack2_a[0][0]               
__________________________________________________________________________________________________
res_stack2_blockb_u1 (Conv1D)   (None, 128, 32)      8224        bn_stack2_a[0][0]                
__________________________________________________________________________________________________
bn_stack2_blockb_u1 (BatchNorma (None, 128, 32)      128         res_stack2_blockb_u1[0][0]       
__________________________________________________________________________________________________
activation_26 (Activation)      (None, 128, 32)      0           bn_stack2_blockb_u1[0][0]        
__________________________________________________________________________________________________
res_stack2_blockb_u2 (Conv1D)   (None, 128, 32)      1056        activation_26[0][0]              
__________________________________________________________________________________________________
bn_stack2_blockb_u2 (BatchNorma (None, 128, 32)      128         res_stack2_blockb_u2[0][0]       
__________________________________________________________________________________________________
add_13 (Add)                    (None, 128, 32)      0           bn_stack2_blockb_u2[0][0]        
                                                                 bn_stack2_a[0][0]                
__________________________________________________________________________________________________
activation_27 (Activation)      (None, 128, 32)      0           add_13[0][0]                     
__________________________________________________________________________________________________
res_stack2_blockc_u1 (Conv1D)   (None, 128, 32)      8224        activation_27[0][0]              
__________________________________________________________________________________________________
bn_stack2_blockc_u1 (BatchNorma (None, 128, 32)      128         res_stack2_blockc_u1[0][0]       
__________________________________________________________________________________________________
activation_28 (Activation)      (None, 128, 32)      0           bn_stack2_blockc_u1[0][0]        
__________________________________________________________________________________________________
res_stack2_blockc_u2 (Conv1D)   (None, 128, 32)      1056        activation_28[0][0]              
__________________________________________________________________________________________________
bn_stack2_blockc_u2 (BatchNorma (None, 128, 32)      128         res_stack2_blockc_u2[0][0]       
__________________________________________________________________________________________________
add_14 (Add)                    (None, 128, 32)      0           bn_stack2_blockc_u2[0][0]        
                                                                 activation_27[0][0]              
__________________________________________________________________________________________________
activation_29 (Activation)      (None, 128, 32)      0           add_14[0][0]                     
__________________________________________________________________________________________________
mp_stack2 (MaxPooling1D)        (None, 128, 32)      0           activation_29[0][0]              
__________________________________________________________________________________________________
pooling_size (MaxPooling1D)     (None, 64, 32)       0           mp_stack2[0][0]                  
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 2048)         0           pooling_size[0][0]               
__________________________________________________________________________________________________
small_dense3 (Dense)            (None, 128)          262272      flatten_1[0][0]                  
__________________________________________________________________________________________________
small_dense4 (Dense)            (None, 10)           1290        small_dense3[0][0]               
__________________________________________________________________________________________________
T (Lambda)                      (None, 10)           0           small_dense4[0][0]               
__________________________________________________________________________________________________
activation_30 (Activation)      (None, 10)           0           T[0][0]                          
==================================================================================================
Total params: 288,346
Trainable params: 287,866
Non-trainable params: 480
__________________________________________________________________________________________________
Train on 960000 samples, validate on 240000 samples
Epoch 1/150
 - 58s - loss: 1.7550 - acc: 0.3226 - val_loss: 3.1507 - val_acc: 0.2708
Epoch 2/150
 - 52s - loss: 1.1156 - acc: 0.5303 - val_loss: 6.0236 - val_acc: 0.1753
Epoch 3/150
 - 53s - loss: 1.0712 - acc: 0.5470 - val_loss: 1.9174 - val_acc: 0.3465
Epoch 4/150
 - 53s - loss: 1.0508 - acc: 0.5552 - val_loss: 3.3543 - val_acc: 0.3130
Epoch 5/150
 - 54s - loss: 1.0347 - acc: 0.5618 - val_loss: 1.5433 - val_acc: 0.4158
Epoch 6/150
 - 53s - loss: 1.0247 - acc: 0.5662 - val_loss: 6.0165 - val_acc: 0.2123
Epoch 7/150
 - 53s - loss: 1.0180 - acc: 0.5686 - val_loss: 6.5027 - val_acc: 0.1723
Epoch 8/150
 - 52s - loss: 1.0124 - acc: 0.5715 - val_loss: 4.9253 - val_acc: 0.2141
Epoch 9/150
 - 52s - loss: 1.0085 - acc: 0.5728 - val_loss: 7.3078 - val_acc: 0.1965
Epoch 10/150
 - 52s - loss: 1.0058 - acc: 0.5739 - val_loss: 5.1576 - val_acc: 0.1895
Epoch 11/150
 - 52s - loss: 1.0028 - acc: 0.5755 - val_loss: 4.9846 - val_acc: 0.2470
Epoch 12/150
 - 52s - loss: 1.0005 - acc: 0.5765 - val_loss: 3.4192 - val_acc: 0.2934
Epoch 13/150
 - 52s - loss: 0.9986 - acc: 0.5776 - val_loss: 4.5355 - val_acc: 0.2407
Epoch 14/150
 - 52s - loss: 0.9951 - acc: 0.5794 - val_loss: 3.3461 - val_acc: 0.2789
Epoch 15/150
 - 53s - loss: 0.9949 - acc: 0.5798 - val_loss: 4.5031 - val_acc: 0.2408
Epoch 16/150
 - 52s - loss: 0.9907 - acc: 0.5826 - val_loss: 4.2171 - val_acc: 0.2701
Epoch 17/150
 - 52s - loss: 0.9888 - acc: 0.5847 - val_loss: 1.5766 - val_acc: 0.4583
Epoch 18/150
 - 52s - loss: 0.9849 - acc: 0.5900 - val_loss: 7.6931 - val_acc: 0.1662
Epoch 19/150
 - 51s - loss: 0.9795 - acc: 0.5965 - val_loss: 5.6274 - val_acc: 0.2290
Epoch 20/150
 - 51s - loss: 0.9715 - acc: 0.6033 - val_loss: 2.1662 - val_acc: 0.3842
Epoch 21/150
 - 52s - loss: 0.9649 - acc: 0.6079 - val_loss: 1.5452 - val_acc: 0.4903
Epoch 22/150
 - 51s - loss: 0.9621 - acc: 0.6099 - val_loss: 4.2816 - val_acc: 0.2687
Epoch 23/150
 - 51s - loss: 0.9562 - acc: 0.6127 - val_loss: 1.2605 - val_acc: 0.4945
Epoch 24/150
 - 51s - loss: 0.9536 - acc: 0.6142 - val_loss: 8.2448 - val_acc: 0.1609
Epoch 25/150
 - 52s - loss: 0.9505 - acc: 0.6157 - val_loss: 9.0393 - val_acc: 0.1623
Epoch 26/150
 - 52s - loss: 0.9482 - acc: 0.6166 - val_loss: 7.8876 - val_acc: 0.1649
Epoch 27/150
 - 52s - loss: 0.9459 - acc: 0.6176 - val_loss: 6.5448 - val_acc: 0.2067
Epoch 28/150
 - 51s - loss: 0.9446 - acc: 0.6183 - val_loss: 1.1809 - val_acc: 0.5415
Epoch 29/150
 - 52s - loss: 0.9419 - acc: 0.6197 - val_loss: 9.1374 - val_acc: 0.1676
Epoch 30/150
 - 52s - loss: 0.9395 - acc: 0.6209 - val_loss: 1.6399 - val_acc: 0.4978
Epoch 31/150
 - 52s - loss: 0.9383 - acc: 0.6210 - val_loss: 7.0647 - val_acc: 0.1846
Epoch 32/150
 - 51s - loss: 0.9367 - acc: 0.6222 - val_loss: 6.8052 - val_acc: 0.1942
Epoch 33/150
 - 51s - loss: 0.9344 - acc: 0.6232 - val_loss: 6.1284 - val_acc: 0.2440
Epoch 34/150
 - 52s - loss: 0.9332 - acc: 0.6234 - val_loss: 2.2157 - val_acc: 0.4115
Epoch 35/150
 - 51s - loss: 0.9321 - acc: 0.6242 - val_loss: 8.5116 - val_acc: 0.1718
Epoch 36/150
 - 52s - loss: 0.9323 - acc: 0.6242 - val_loss: 9.3482 - val_acc: 0.1577
Epoch 37/150
 - 52s - loss: 0.9290 - acc: 0.6257 - val_loss: 6.3029 - val_acc: 0.2373
Epoch 38/150
 - 52s - loss: 0.9286 - acc: 0.6261 - val_loss: 5.1524 - val_acc: 0.2459
Epoch 39/150
 - 52s - loss: 0.9266 - acc: 0.6263 - val_loss: 3.3865 - val_acc: 0.3209
Epoch 40/150
 - 51s - loss: 0.9259 - acc: 0.6267 - val_loss: 6.9599 - val_acc: 0.1873
Epoch 41/150
 - 52s - loss: 0.9252 - acc: 0.6276 - val_loss: 9.8132 - val_acc: 0.1367
Epoch 42/150
 - 51s - loss: 0.9228 - acc: 0.6281 - val_loss: 2.8260 - val_acc: 0.3572
Epoch 43/150
 - 51s - loss: 0.9216 - acc: 0.6290 - val_loss: 7.7967 - val_acc: 0.1720
Epoch 44/150
 - 52s - loss: 0.9215 - acc: 0.6291 - val_loss: 4.3281 - val_acc: 0.2981
Epoch 45/150
 - 52s - loss: 0.9199 - acc: 0.6294 - val_loss: 4.2488 - val_acc: 0.2629
Epoch 46/150
 - 52s - loss: 0.9191 - acc: 0.6302 - val_loss: 2.2299 - val_acc: 0.4133
Epoch 47/150
 - 52s - loss: 0.9185 - acc: 0.6304 - val_loss: 4.0313 - val_acc: 0.3284
Epoch 48/150
 - 52s - loss: 0.9166 - acc: 0.6313 - val_loss: 1.3236 - val_acc: 0.5288
Epoch 49/150
 - 52s - loss: 0.9156 - acc: 0.6316 - val_loss: 3.2824 - val_acc: 0.3497
Epoch 50/150
 - 52s - loss: 0.9152 - acc: 0.6314 - val_loss: 7.6939 - val_acc: 0.1900
Epoch 51/150
 - 52s - loss: 0.9146 - acc: 0.6316 - val_loss: 8.0265 - val_acc: 0.1799
Epoch 52/150
 - 51s - loss: 0.9122 - acc: 0.6331 - val_loss: 1.4312 - val_acc: 0.5243
Epoch 53/150
 - 52s - loss: 0.9115 - acc: 0.6334 - val_loss: 8.1782 - val_acc: 0.1714
Epoch 54/150
 - 52s - loss: 0.9115 - acc: 0.6330 - val_loss: 4.3240 - val_acc: 0.2901
Epoch 55/150
 - 52s - loss: 0.9094 - acc: 0.6340 - val_loss: 5.2307 - val_acc: 0.2659
Epoch 56/150
 - 52s - loss: 0.9086 - acc: 0.6342 - val_loss: 8.8343 - val_acc: 0.1816
Epoch 57/150
 - 51s - loss: 0.9080 - acc: 0.6347 - val_loss: 2.0132 - val_acc: 0.4222
Epoch 58/150
 - 52s - loss: 0.9076 - acc: 0.6349 - val_loss: 9.3550 - val_acc: 0.1477
Epoch 59/150
 - 52s - loss: 0.9063 - acc: 0.6356 - val_loss: 10.0968 - val_acc: 0.1385
Epoch 60/150
 - 51s - loss: 0.9065 - acc: 0.6351 - val_loss: 2.6590 - val_acc: 0.3523
Epoch 61/150
 - 52s - loss: 0.9044 - acc: 0.6361 - val_loss: 5.5307 - val_acc: 0.2587
Epoch 62/150
 - 52s - loss: 0.9038 - acc: 0.6365 - val_loss: 6.3533 - val_acc: 0.2192
Epoch 63/150
 - 51s - loss: 0.9033 - acc: 0.6365 - val_loss: 2.1555 - val_acc: 0.4342
Epoch 64/150
 - 52s - loss: 0.9032 - acc: 0.6366 - val_loss: 7.4151 - val_acc: 0.2042
Epoch 65/150
 - 52s - loss: 0.9026 - acc: 0.6366 - val_loss: 4.1476 - val_acc: 0.3234
Epoch 66/150
 - 52s - loss: 0.9016 - acc: 0.6373 - val_loss: 4.5621 - val_acc: 0.2673
Epoch 67/150
 - 51s - loss: 0.9012 - acc: 0.6374 - val_loss: 3.7913 - val_acc: 0.3347
Epoch 68/150
 - 52s - loss: 0.9003 - acc: 0.6376 - val_loss: 2.0795 - val_acc: 0.4112
Epoch 69/150
 - 51s - loss: 0.9005 - acc: 0.6375 - val_loss: 7.2685 - val_acc: 0.1735
Epoch 70/150
 - 52s - loss: 0.9000 - acc: 0.6377 - val_loss: 2.4353 - val_acc: 0.4078
Epoch 71/150
 - 52s - loss: 0.8999 - acc: 0.6378 - val_loss: 8.6007 - val_acc: 0.1552
Epoch 72/150
 - 52s - loss: 0.8989 - acc: 0.6384 - val_loss: 4.8582 - val_acc: 0.2850
Epoch 73/150
 - 51s - loss: 0.8980 - acc: 0.6387 - val_loss: 4.5086 - val_acc: 0.2892
Epoch 74/150
 - 52s - loss: 0.8973 - acc: 0.6387 - val_loss: 5.3491 - val_acc: 0.2805
Epoch 75/150
 - 52s - loss: 0.8973 - acc: 0.6388 - val_loss: 6.1890 - val_acc: 0.2390
Epoch 76/150
 - 52s - loss: 0.8964 - acc: 0.6392 - val_loss: 4.1499 - val_acc: 0.3065
Epoch 77/150
 - 52s - loss: 0.8963 - acc: 0.6393 - val_loss: 1.1920 - val_acc: 0.5381
Epoch 78/150
 - 52s - loss: 0.8960 - acc: 0.6395 - val_loss: 2.9845 - val_acc: 0.4150
Epoch 79/150
 - 52s - loss: 0.8961 - acc: 0.6394 - val_loss: 4.4246 - val_acc: 0.3046
Epoch 80/150
 - 51s - loss: 0.8944 - acc: 0.6399 - val_loss: 1.0931 - val_acc: 0.5626
Epoch 81/150
 - 51s - loss: 0.8948 - acc: 0.6397 - val_loss: 2.1372 - val_acc: 0.4300
Epoch 82/150
 - 52s - loss: 0.8940 - acc: 0.6398 - val_loss: 5.6482 - val_acc: 0.2502
Epoch 83/150
 - 52s - loss: 0.8935 - acc: 0.6405 - val_loss: 10.0035 - val_acc: 0.1526
Epoch 84/150
 - 52s - loss: 0.8936 - acc: 0.6405 - val_loss: 1.3626 - val_acc: 0.5175
Epoch 85/150
 - 52s - loss: 0.8923 - acc: 0.6406 - val_loss: 2.4258 - val_acc: 0.3760
Epoch 86/150
 - 52s - loss: 0.8927 - acc: 0.6402 - val_loss: 7.2826 - val_acc: 0.2049
Epoch 87/150
 - 52s - loss: 0.8918 - acc: 0.6409 - val_loss: 5.2880 - val_acc: 0.2184
Epoch 88/150
 - 52s - loss: 0.8911 - acc: 0.6411 - val_loss: 9.0960 - val_acc: 0.1665
Epoch 89/150
 - 52s - loss: 0.8906 - acc: 0.6416 - val_loss: 5.2854 - val_acc: 0.2684
Epoch 90/150
 - 51s - loss: 0.8912 - acc: 0.6412 - val_loss: 5.0947 - val_acc: 0.2645
Epoch 91/150
 - 51s - loss: 0.8904 - acc: 0.6418 - val_loss: 3.8175 - val_acc: 0.3200
Epoch 92/150
 - 52s - loss: 0.8904 - acc: 0.6416 - val_loss: 1.1324 - val_acc: 0.5794
Epoch 93/150
 - 52s - loss: 0.8896 - acc: 0.6417 - val_loss: 3.9557 - val_acc: 0.3059
Epoch 94/150
 - 52s - loss: 0.8893 - acc: 0.6420 - val_loss: 1.8579 - val_acc: 0.4472
Epoch 95/150
 - 52s - loss: 0.8891 - acc: 0.6423 - val_loss: 8.9524 - val_acc: 0.1744
Epoch 96/150
 - 51s - loss: 0.8893 - acc: 0.6419 - val_loss: 6.3402 - val_acc: 0.2303
Epoch 97/150
 - 52s - loss: 0.8884 - acc: 0.6424 - val_loss: 4.6923 - val_acc: 0.3060
Epoch 98/150
 - 52s - loss: 0.8885 - acc: 0.6423 - val_loss: 8.0426 - val_acc: 0.1785
Epoch 99/150
 - 52s - loss: 0.8883 - acc: 0.6426 - val_loss: 4.5566 - val_acc: 0.2816
Epoch 100/150
 - 52s - loss: 0.8875 - acc: 0.6429 - val_loss: 6.9416 - val_acc: 0.2101
Epoch 101/150
 - 52s - loss: 0.8872 - acc: 0.6428 - val_loss: 1.2826 - val_acc: 0.5462
Epoch 102/150
 - 52s - loss: 0.8866 - acc: 0.6431 - val_loss: 3.0066 - val_acc: 0.3790
Epoch 103/150
 - 51s - loss: 0.8865 - acc: 0.6431 - val_loss: 8.5575 - val_acc: 0.1725
Epoch 104/150
 - 53s - loss: 0.8868 - acc: 0.6430 - val_loss: 1.8527 - val_acc: 0.4531
Epoch 105/150
 - 53s - loss: 0.8858 - acc: 0.6433 - val_loss: 6.5906 - val_acc: 0.2247
Epoch 106/150
 - 52s - loss: 0.8861 - acc: 0.6431 - val_loss: 5.4609 - val_acc: 0.2710
Epoch 107/150
 - 51s - loss: 0.8855 - acc: 0.6434 - val_loss: 2.7561 - val_acc: 0.3737
Epoch 108/150
 - 51s - loss: 0.8852 - acc: 0.6435 - val_loss: 1.8129 - val_acc: 0.4540
Epoch 109/150
 - 50s - loss: 0.8854 - acc: 0.6437 - val_loss: 1.6263 - val_acc: 0.5083
Epoch 110/150
 - 51s - loss: 0.8847 - acc: 0.6440 - val_loss: 5.8501 - val_acc: 0.2398
Epoch 111/150
 - 51s - loss: 0.8845 - acc: 0.6441 - val_loss: 2.4012 - val_acc: 0.3956
Epoch 112/150
 - 51s - loss: 0.8835 - acc: 0.6442 - val_loss: 1.4550 - val_acc: 0.5562
Epoch 113/150
 - 52s - loss: 0.8835 - acc: 0.6444 - val_loss: 7.8558 - val_acc: 0.1837
Epoch 114/150
 - 51s - loss: 0.8844 - acc: 0.6437 - val_loss: 6.1858 - val_acc: 0.2540
Epoch 115/150
 - 51s - loss: 0.8836 - acc: 0.6440 - val_loss: 8.9850 - val_acc: 0.1685
Epoch 116/150
 - 51s - loss: 0.8831 - acc: 0.6443 - val_loss: 8.2901 - val_acc: 0.1796
Epoch 117/150
 - 51s - loss: 0.8826 - acc: 0.6449 - val_loss: 4.5566 - val_acc: 0.2979
Epoch 118/150
 - 51s - loss: 0.8830 - acc: 0.6446 - val_loss: 1.0947 - val_acc: 0.5900
Epoch 119/150
 - 52s - loss: 0.8822 - acc: 0.6444 - val_loss: 6.7084 - val_acc: 0.2089
Epoch 120/150
 - 52s - loss: 0.8824 - acc: 0.6446 - val_loss: 6.9827 - val_acc: 0.2060
Epoch 121/150
 - 51s - loss: 0.8824 - acc: 0.6446 - val_loss: 6.8263 - val_acc: 0.2238
Epoch 122/150
 - 51s - loss: 0.8815 - acc: 0.6451 - val_loss: 5.9748 - val_acc: 0.2328
Epoch 123/150
 - 51s - loss: 0.8822 - acc: 0.6447 - val_loss: 8.0926 - val_acc: 0.1907
Epoch 124/150
 - 51s - loss: 0.8814 - acc: 0.6453 - val_loss: 1.6650 - val_acc: 0.5126
Epoch 125/150
 - 52s - loss: 0.8812 - acc: 0.6452 - val_loss: 10.5660 - val_acc: 0.1475
Epoch 126/150
 - 51s - loss: 0.8802 - acc: 0.6458 - val_loss: 7.7646 - val_acc: 0.1767
Epoch 127/150
 - 51s - loss: 0.8808 - acc: 0.6455 - val_loss: 1.4597 - val_acc: 0.5508
Epoch 128/150
 - 51s - loss: 0.8806 - acc: 0.6454 - val_loss: 8.5652 - val_acc: 0.1770
Epoch 129/150
 - 51s - loss: 0.8803 - acc: 0.6452 - val_loss: 5.2551 - val_acc: 0.2647
Epoch 130/150
 - 51s - loss: 0.8806 - acc: 0.6454 - val_loss: 6.6929 - val_acc: 0.2254
Epoch 131/150
 - 51s - loss: 0.8798 - acc: 0.6456 - val_loss: 9.2626 - val_acc: 0.1802
Epoch 132/150
 - 51s - loss: 0.8792 - acc: 0.6457 - val_loss: 8.2611 - val_acc: 0.1627
Epoch 133/150
 - 51s - loss: 0.8792 - acc: 0.6461 - val_loss: 9.1303 - val_acc: 0.1668
Epoch 134/150
 - 51s - loss: 0.8791 - acc: 0.6460 - val_loss: 5.3377 - val_acc: 0.2620
Epoch 135/150
 - 52s - loss: 0.8790 - acc: 0.6460 - val_loss: 5.2311 - val_acc: 0.2408
Epoch 136/150
 - 51s - loss: 0.8783 - acc: 0.6464 - val_loss: 7.8282 - val_acc: 0.2119
Epoch 137/150
 - 50s - loss: 0.8793 - acc: 0.6457 - val_loss: 6.4190 - val_acc: 0.2177
Epoch 138/150
 - 50s - loss: 0.8785 - acc: 0.6460 - val_loss: 8.6364 - val_acc: 0.1820
Epoch 139/150
 - 52s - loss: 0.8777 - acc: 0.6465 - val_loss: 1.2689 - val_acc: 0.5601
Epoch 140/150
 - 52s - loss: 0.8780 - acc: 0.6465 - val_loss: 9.4682 - val_acc: 0.1529
Epoch 141/150
 - 51s - loss: 0.8772 - acc: 0.6467 - val_loss: 3.5028 - val_acc: 0.3430
Epoch 142/150
 - 51s - loss: 0.8772 - acc: 0.6469 - val_loss: 5.5992 - val_acc: 0.2773
Epoch 143/150
 - 50s - loss: 0.8770 - acc: 0.6469 - val_loss: 8.6942 - val_acc: 0.1643
Epoch 144/150
 - 52s - loss: 0.8766 - acc: 0.6468 - val_loss: 8.2313 - val_acc: 0.1932
Epoch 145/150
 - 51s - loss: 0.8771 - acc: 0.6468 - val_loss: 2.9607 - val_acc: 0.3824
Epoch 146/150
 - 51s - loss: 0.8761 - acc: 0.6470 - val_loss: 5.5996 - val_acc: 0.2702
Epoch 147/150
 - 52s - loss: 0.8761 - acc: 0.6470 - val_loss: 6.8839 - val_acc: 0.2177
Epoch 148/150
 - 52s - loss: 0.8755 - acc: 0.6470 - val_loss: 6.1353 - val_acc: 0.2466
Epoch 149/150
 - 51s - loss: 0.8758 - acc: 0.6469 - val_loss: 3.6098 - val_acc: 0.3362
Epoch 150/150
 - 52s - loss: 0.8757 - acc: 0.6471 - val_loss: 6.0436 - val_acc: 0.2278
('small_ResNetLSTMLikeModel:', [1.0930526223182677, 0.56263750003178914])
start to predict
-------------------------------------------------------------------------------------------------------------------NSM trained in the CML way
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
Input (InputLayer)              (None, 128, 2)       0                                            
__________________________________________________________________________________________________
res_stack1_a (Conv1D)           (None, 128, 16)      48          Input[0][0]                      
__________________________________________________________________________________________________
bn_stack1_a (BatchNormalization (None, 128, 16)      64          res_stack1_a[0][0]               
__________________________________________________________________________________________________
res_stack1_blockb_u1 (Conv1D)   (None, 128, 16)      2064        bn_stack1_a[0][0]                
__________________________________________________________________________________________________
bn_stack1_blockb_u1 (BatchNorma (None, 128, 16)      64          res_stack1_blockb_u1[0][0]       
__________________________________________________________________________________________________
activation_31 (Activation)      (None, 128, 16)      0           bn_stack1_blockb_u1[0][0]        
__________________________________________________________________________________________________
res_stack1_blockb_u2 (Conv1D)   (None, 128, 16)      272         activation_31[0][0]              
__________________________________________________________________________________________________
bn_stack1_blockb_u2 (BatchNorma (None, 128, 16)      64          res_stack1_blockb_u2[0][0]       
__________________________________________________________________________________________________
add_15 (Add)                    (None, 128, 16)      0           bn_stack1_blockb_u2[0][0]        
                                                                 bn_stack1_a[0][0]                
__________________________________________________________________________________________________
activation_32 (Activation)      (None, 128, 16)      0           add_15[0][0]                     
__________________________________________________________________________________________________
res_stack1_blockc_u1 (Conv1D)   (None, 128, 16)      2064        activation_32[0][0]              
__________________________________________________________________________________________________
bn_stack1_blockc_u1 (BatchNorma (None, 128, 16)      64          res_stack1_blockc_u1[0][0]       
__________________________________________________________________________________________________
activation_33 (Activation)      (None, 128, 16)      0           bn_stack1_blockc_u1[0][0]        
__________________________________________________________________________________________________
res_stack1_blockc_u2 (Conv1D)   (None, 128, 16)      272         activation_33[0][0]              
__________________________________________________________________________________________________
bn_stack1_blockc_u2 (BatchNorma (None, 128, 16)      64          res_stack1_blockc_u2[0][0]       
__________________________________________________________________________________________________
add_16 (Add)                    (None, 128, 16)      0           bn_stack1_blockc_u2[0][0]        
                                                                 activation_32[0][0]              
__________________________________________________________________________________________________
activation_34 (Activation)      (None, 128, 16)      0           add_16[0][0]                     
__________________________________________________________________________________________________
mp_stack1 (MaxPooling1D)        (None, 128, 16)      0           activation_34[0][0]              
__________________________________________________________________________________________________
res_stack2_a (Conv1D)           (None, 128, 32)      544         mp_stack1[0][0]                  
__________________________________________________________________________________________________
bn_stack2_a (BatchNormalization (None, 128, 32)      128         res_stack2_a[0][0]               
__________________________________________________________________________________________________
res_stack2_blockb_u1 (Conv1D)   (None, 128, 32)      8224        bn_stack2_a[0][0]                
__________________________________________________________________________________________________
bn_stack2_blockb_u1 (BatchNorma (None, 128, 32)      128         res_stack2_blockb_u1[0][0]       
__________________________________________________________________________________________________
activation_35 (Activation)      (None, 128, 32)      0           bn_stack2_blockb_u1[0][0]        
__________________________________________________________________________________________________
res_stack2_blockb_u2 (Conv1D)   (None, 128, 32)      1056        activation_35[0][0]              
__________________________________________________________________________________________________
bn_stack2_blockb_u2 (BatchNorma (None, 128, 32)      128         res_stack2_blockb_u2[0][0]       
__________________________________________________________________________________________________
add_17 (Add)                    (None, 128, 32)      0           bn_stack2_blockb_u2[0][0]        
                                                                 bn_stack2_a[0][0]                
__________________________________________________________________________________________________
activation_36 (Activation)      (None, 128, 32)      0           add_17[0][0]                     
__________________________________________________________________________________________________
res_stack2_blockc_u1 (Conv1D)   (None, 128, 32)      8224        activation_36[0][0]              
__________________________________________________________________________________________________
bn_stack2_blockc_u1 (BatchNorma (None, 128, 32)      128         res_stack2_blockc_u1[0][0]       
__________________________________________________________________________________________________
activation_37 (Activation)      (None, 128, 32)      0           bn_stack2_blockc_u1[0][0]        
__________________________________________________________________________________________________
res_stack2_blockc_u2 (Conv1D)   (None, 128, 32)      1056        activation_37[0][0]              
__________________________________________________________________________________________________
bn_stack2_blockc_u2 (BatchNorma (None, 128, 32)      128         res_stack2_blockc_u2[0][0]       
__________________________________________________________________________________________________
add_18 (Add)                    (None, 128, 32)      0           bn_stack2_blockc_u2[0][0]        
                                                                 activation_36[0][0]              
__________________________________________________________________________________________________
activation_38 (Activation)      (None, 128, 32)      0           add_18[0][0]                     
__________________________________________________________________________________________________
mp_stack2 (MaxPooling1D)        (None, 128, 32)      0           activation_38[0][0]              
__________________________________________________________________________________________________
res_stack3_a (Conv1D)           (None, 128, 48)      1584        mp_stack2[0][0]                  
__________________________________________________________________________________________________
bn_stack3_a (BatchNormalization (None, 128, 48)      192         res_stack3_a[0][0]               
__________________________________________________________________________________________________
res_stack3_blockb_u1 (Conv1D)   (None, 128, 48)      18480       bn_stack3_a[0][0]                
__________________________________________________________________________________________________
bn_stack3_blockb_u1 (BatchNorma (None, 128, 48)      192         res_stack3_blockb_u1[0][0]       
__________________________________________________________________________________________________
activation_39 (Activation)      (None, 128, 48)      0           bn_stack3_blockb_u1[0][0]        
__________________________________________________________________________________________________
res_stack3_blockb_u2 (Conv1D)   (None, 128, 48)      2352        activation_39[0][0]              
__________________________________________________________________________________________________
bn_stack3_blockb_u2 (BatchNorma (None, 128, 48)      192         res_stack3_blockb_u2[0][0]       
__________________________________________________________________________________________________
add_19 (Add)                    (None, 128, 48)      0           bn_stack3_blockb_u2[0][0]        
                                                                 bn_stack3_a[0][0]                
__________________________________________________________________________________________________
activation_40 (Activation)      (None, 128, 48)      0           add_19[0][0]                     
__________________________________________________________________________________________________
res_stack3_blockc_u1 (Conv1D)   (None, 128, 48)      18480       activation_40[0][0]              
__________________________________________________________________________________________________
bn_stack3_blockc_u1 (BatchNorma (None, 128, 48)      192         res_stack3_blockc_u1[0][0]       
__________________________________________________________________________________________________
activation_41 (Activation)      (None, 128, 48)      0           bn_stack3_blockc_u1[0][0]        
__________________________________________________________________________________________________
res_stack3_blockc_u2 (Conv1D)   (None, 128, 48)      2352        activation_41[0][0]              
__________________________________________________________________________________________________
bn_stack3_blockc_u2 (BatchNorma (None, 128, 48)      192         res_stack3_blockc_u2[0][0]       
__________________________________________________________________________________________________
add_20 (Add)                    (None, 128, 48)      0           bn_stack3_blockc_u2[0][0]        
                                                                 activation_40[0][0]              
__________________________________________________________________________________________________
activation_42 (Activation)      (None, 128, 48)      0           add_20[0][0]                     
__________________________________________________________________________________________________
mp_stack3 (MaxPooling1D)        (None, 128, 48)      0           activation_42[0][0]              
__________________________________________________________________________________________________
res_stack4_a (Conv1D)           (None, 128, 64)      3136        mp_stack3[0][0]                  
__________________________________________________________________________________________________
bn_stack4_a (BatchNormalization (None, 128, 64)      256         res_stack4_a[0][0]               
__________________________________________________________________________________________________
res_stack4_blockb_u1 (Conv1D)   (None, 128, 64)      32832       bn_stack4_a[0][0]                
__________________________________________________________________________________________________
bn_stack4_blockb_u1 (BatchNorma (None, 128, 64)      256         res_stack4_blockb_u1[0][0]       
__________________________________________________________________________________________________
activation_43 (Activation)      (None, 128, 64)      0           bn_stack4_blockb_u1[0][0]        
__________________________________________________________________________________________________
res_stack4_blockb_u2 (Conv1D)   (None, 128, 64)      4160        activation_43[0][0]              
__________________________________________________________________________________________________
bn_stack4_blockb_u2 (BatchNorma (None, 128, 64)      256         res_stack4_blockb_u2[0][0]       
__________________________________________________________________________________________________
add_21 (Add)                    (None, 128, 64)      0           bn_stack4_blockb_u2[0][0]        
                                                                 bn_stack4_a[0][0]                
__________________________________________________________________________________________________
activation_44 (Activation)      (None, 128, 64)      0           add_21[0][0]                     
__________________________________________________________________________________________________
res_stack4_blockc_u1 (Conv1D)   (None, 128, 64)      32832       activation_44[0][0]              
__________________________________________________________________________________________________
bn_stack4_blockc_u1 (BatchNorma (None, 128, 64)      256         res_stack4_blockc_u1[0][0]       
__________________________________________________________________________________________________
activation_45 (Activation)      (None, 128, 64)      0           bn_stack4_blockc_u1[0][0]        
__________________________________________________________________________________________________
res_stack4_blockc_u2 (Conv1D)   (None, 128, 64)      4160        activation_45[0][0]              
__________________________________________________________________________________________________
bn_stack4_blockc_u2 (BatchNorma (None, 128, 64)      256         res_stack4_blockc_u2[0][0]       
__________________________________________________________________________________________________
add_22 (Add)                    (None, 128, 64)      0           bn_stack4_blockc_u2[0][0]        
                                                                 activation_44[0][0]              
__________________________________________________________________________________________________
activation_46 (Activation)      (None, 128, 64)      0           add_22[0][0]                     
__________________________________________________________________________________________________
mp_stack4 (MaxPooling1D)        (None, 128, 64)      0           activation_46[0][0]              
__________________________________________________________________________________________________
res_stack5_a (Conv1D)           (None, 128, 80)      5200        mp_stack4[0][0]                  
__________________________________________________________________________________________________
bn_stack5_a (BatchNormalization (None, 128, 80)      320         res_stack5_a[0][0]               
__________________________________________________________________________________________________
res_stack5_blockb_u1 (Conv1D)   (None, 128, 80)      51280       bn_stack5_a[0][0]                
__________________________________________________________________________________________________
bn_stack5_blockb_u1 (BatchNorma (None, 128, 80)      320         res_stack5_blockb_u1[0][0]       
__________________________________________________________________________________________________
activation_47 (Activation)      (None, 128, 80)      0           bn_stack5_blockb_u1[0][0]        
__________________________________________________________________________________________________
res_stack5_blockb_u2 (Conv1D)   (None, 128, 80)      6480        activation_47[0][0]              
__________________________________________________________________________________________________
bn_stack5_blockb_u2 (BatchNorma (None, 128, 80)      320         res_stack5_blockb_u2[0][0]       
__________________________________________________________________________________________________
add_23 (Add)                    (None, 128, 80)      0           bn_stack5_blockb_u2[0][0]        
                                                                 bn_stack5_a[0][0]                
__________________________________________________________________________________________________
activation_48 (Activation)      (None, 128, 80)      0           add_23[0][0]                     
__________________________________________________________________________________________________
res_stack5_blockc_u1 (Conv1D)   (None, 128, 80)      51280       activation_48[0][0]              
__________________________________________________________________________________________________
bn_stack5_blockc_u1 (BatchNorma (None, 128, 80)      320         res_stack5_blockc_u1[0][0]       
__________________________________________________________________________________________________
activation_49 (Activation)      (None, 128, 80)      0           bn_stack5_blockc_u1[0][0]        
__________________________________________________________________________________________________
res_stack5_blockc_u2 (Conv1D)   (None, 128, 80)      6480        activation_49[0][0]              
__________________________________________________________________________________________________
bn_stack5_blockc_u2 (BatchNorma (None, 128, 80)      320         res_stack5_blockc_u2[0][0]       
__________________________________________________________________________________________________
add_24 (Add)                    (None, 128, 80)      0           bn_stack5_blockc_u2[0][0]        
                                                                 activation_48[0][0]              
__________________________________________________________________________________________________
activation_50 (Activation)      (None, 128, 80)      0           add_24[0][0]                     
__________________________________________________________________________________________________
mp_stack5 (MaxPooling1D)        (None, 128, 80)      0           activation_50[0][0]              
__________________________________________________________________________________________________
cu_dnnlstm_2 (CuDNNLSTM)        (None, 50)           26400       mp_stack5[0][0]                  
__________________________________________________________________________________________________
dense1 (Dense)                  (None, 128)          6528        cu_dnnlstm_2[0][0]               
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 128)          0           dense1[0][0]                     
__________________________________________________________________________________________________
dense3 (Dense)                  (None, 10)           1290        dropout_2[0][0]                  
__________________________________________________________________________________________________
T (Lambda)                      (None, 10)           0           dense3[0][0]                     
__________________________________________________________________________________________________
activation_51 (Activation)      (None, 10)           0           T[0][0]                          
==================================================================================================
Total params: 303,930
Trainable params: 301,530
Non-trainable params: 2,400
__________________________________________________________________________________________________
end to predict
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
Input (InputLayer)              (None, 128, 2)       0                                            
__________________________________________________________________________________________________
res_stack1_a (Conv1D)           (None, 128, 16)      48          Input[0][0]                      
__________________________________________________________________________________________________
bn_stack1_a (BatchNormalization (None, 128, 16)      64          res_stack1_a[0][0]               
__________________________________________________________________________________________________
res_stack1_blockb_u1 (Conv1D)   (None, 128, 16)      2064        bn_stack1_a[0][0]                
__________________________________________________________________________________________________
bn_stack1_blockb_u1 (BatchNorma (None, 128, 16)      64          res_stack1_blockb_u1[0][0]       
__________________________________________________________________________________________________
activation_52 (Activation)      (None, 128, 16)      0           bn_stack1_blockb_u1[0][0]        
__________________________________________________________________________________________________
res_stack1_blockb_u2 (Conv1D)   (None, 128, 16)      272         activation_52[0][0]              
__________________________________________________________________________________________________
bn_stack1_blockb_u2 (BatchNorma (None, 128, 16)      64          res_stack1_blockb_u2[0][0]       
__________________________________________________________________________________________________
add_25 (Add)                    (None, 128, 16)      0           bn_stack1_blockb_u2[0][0]        
                                                                 bn_stack1_a[0][0]                
__________________________________________________________________________________________________
activation_53 (Activation)      (None, 128, 16)      0           add_25[0][0]                     
__________________________________________________________________________________________________
res_stack1_blockc_u1 (Conv1D)   (None, 128, 16)      2064        activation_53[0][0]              
__________________________________________________________________________________________________
bn_stack1_blockc_u1 (BatchNorma (None, 128, 16)      64          res_stack1_blockc_u1[0][0]       
__________________________________________________________________________________________________
activation_54 (Activation)      (None, 128, 16)      0           bn_stack1_blockc_u1[0][0]        
__________________________________________________________________________________________________
res_stack1_blockc_u2 (Conv1D)   (None, 128, 16)      272         activation_54[0][0]              
__________________________________________________________________________________________________
bn_stack1_blockc_u2 (BatchNorma (None, 128, 16)      64          res_stack1_blockc_u2[0][0]       
__________________________________________________________________________________________________
add_26 (Add)                    (None, 128, 16)      0           bn_stack1_blockc_u2[0][0]        
                                                                 activation_53[0][0]              
__________________________________________________________________________________________________
activation_55 (Activation)      (None, 128, 16)      0           add_26[0][0]                     
__________________________________________________________________________________________________
mp_stack1 (MaxPooling1D)        (None, 128, 16)      0           activation_55[0][0]              
__________________________________________________________________________________________________
res_stack2_a (Conv1D)           (None, 128, 32)      544         mp_stack1[0][0]                  
__________________________________________________________________________________________________
bn_stack2_a (BatchNormalization (None, 128, 32)      128         res_stack2_a[0][0]               
__________________________________________________________________________________________________
res_stack2_blockb_u1 (Conv1D)   (None, 128, 32)      8224        bn_stack2_a[0][0]                
__________________________________________________________________________________________________
bn_stack2_blockb_u1 (BatchNorma (None, 128, 32)      128         res_stack2_blockb_u1[0][0]       
__________________________________________________________________________________________________
activation_56 (Activation)      (None, 128, 32)      0           bn_stack2_blockb_u1[0][0]        
__________________________________________________________________________________________________
res_stack2_blockb_u2 (Conv1D)   (None, 128, 32)      1056        activation_56[0][0]              
__________________________________________________________________________________________________
bn_stack2_blockb_u2 (BatchNorma (None, 128, 32)      128         res_stack2_blockb_u2[0][0]       
__________________________________________________________________________________________________
add_27 (Add)                    (None, 128, 32)      0           bn_stack2_blockb_u2[0][0]        
                                                                 bn_stack2_a[0][0]                
__________________________________________________________________________________________________
activation_57 (Activation)      (None, 128, 32)      0           add_27[0][0]                     
__________________________________________________________________________________________________
res_stack2_blockc_u1 (Conv1D)   (None, 128, 32)      8224        activation_57[0][0]              
__________________________________________________________________________________________________
bn_stack2_blockc_u1 (BatchNorma (None, 128, 32)      128         res_stack2_blockc_u1[0][0]       
__________________________________________________________________________________________________
activation_58 (Activation)      (None, 128, 32)      0           bn_stack2_blockc_u1[0][0]        
__________________________________________________________________________________________________
res_stack2_blockc_u2 (Conv1D)   (None, 128, 32)      1056        activation_58[0][0]              
__________________________________________________________________________________________________
bn_stack2_blockc_u2 (BatchNorma (None, 128, 32)      128         res_stack2_blockc_u2[0][0]       
__________________________________________________________________________________________________
add_28 (Add)                    (None, 128, 32)      0           bn_stack2_blockc_u2[0][0]        
                                                                 activation_57[0][0]              
__________________________________________________________________________________________________
activation_59 (Activation)      (None, 128, 32)      0           add_28[0][0]                     
__________________________________________________________________________________________________
mp_stack2 (MaxPooling1D)        (None, 128, 32)      0           activation_59[0][0]              
__________________________________________________________________________________________________
pooling_size (MaxPooling1D)     (None, 64, 32)       0           mp_stack2[0][0]                  
__________________________________________________________________________________________________
flatten_2 (Flatten)             (None, 2048)         0           pooling_size[0][0]               
__________________________________________________________________________________________________
small_dense3 (Dense)            (None, 128)          262272      flatten_2[0][0]                  
__________________________________________________________________________________________________
small_dense4 (Dense)            (None, 10)           1290        small_dense3[0][0]               
__________________________________________________________________________________________________
T (Lambda)                      (None, 10)           0           small_dense4[0][0]               
__________________________________________________________________________________________________
activation_60 (Activation)      (None, 10)           0           T[0][0]                          
==================================================================================================
Total params: 288,346
Trainable params: 287,866
Non-trainable params: 480
__________________________________________________________________________________________________
Train on 960000 samples, validate on 240000 samples
Epoch 1/150
 - 56s - loss: 1.9234 - acc: 0.6886 - val_loss: 1.4460 - val_acc: 0.5975
Epoch 2/150
 - 52s - loss: 1.9151 - acc: 0.6668 - val_loss: 1.4049 - val_acc: 0.6077
Epoch 3/150
 - 51s - loss: 1.9135 - acc: 0.6756 - val_loss: 1.4002 - val_acc: 0.6076
Epoch 4/150
 - 52s - loss: 1.9128 - acc: 0.6823 - val_loss: 1.4010 - val_acc: 0.5983
Epoch 5/150
 - 51s - loss: 1.9123 - acc: 0.6869 - val_loss: 1.3911 - val_acc: 0.5997
Epoch 6/150
 - 51s - loss: 1.9119 - acc: 0.6960 - val_loss: 1.4291 - val_acc: 0.5906
Epoch 7/150
 - 50s - loss: 1.9116 - acc: 0.7046 - val_loss: 1.4315 - val_acc: 0.5926
Epoch 8/150
 - 50s - loss: 1.9113 - acc: 0.7072 - val_loss: 1.4025 - val_acc: 0.6074
Epoch 9/150
 - 51s - loss: 1.9111 - acc: 0.7100 - val_loss: 1.4218 - val_acc: 0.5971
Epoch 10/150
 - 51s - loss: 1.9110 - acc: 0.7107 - val_loss: 1.4356 - val_acc: 0.5782
Epoch 11/150
 - 51s - loss: 1.9106 - acc: 0.7154 - val_loss: 1.4313 - val_acc: 0.5830
Epoch 12/150
 - 51s - loss: 1.9104 - acc: 0.7159 - val_loss: 1.3978 - val_acc: 0.6101
Epoch 13/150
 - 50s - loss: 1.9102 - acc: 0.7202 - val_loss: 1.3942 - val_acc: 0.5998
Epoch 14/150
 - 50s - loss: 1.9101 - acc: 0.7209 - val_loss: 1.4504 - val_acc: 0.5765
Epoch 15/150
 - 52s - loss: 1.9099 - acc: 0.7233 - val_loss: 1.4334 - val_acc: 0.5956
Epoch 16/150
 - 52s - loss: 1.9098 - acc: 0.7238 - val_loss: 1.4384 - val_acc: 0.5851
Epoch 17/150
 - 51s - loss: 1.9097 - acc: 0.7244 - val_loss: 1.4122 - val_acc: 0.6046
Epoch 18/150
 - 52s - loss: 1.9096 - acc: 0.7263 - val_loss: 1.3902 - val_acc: 0.6069
Epoch 19/150
 - 51s - loss: 1.9096 - acc: 0.7248 - val_loss: 1.4119 - val_acc: 0.5910
Epoch 20/150
 - 51s - loss: 1.9095 - acc: 0.7262 - val_loss: 1.3956 - val_acc: 0.6078
Epoch 21/150
 - 51s - loss: 1.9095 - acc: 0.7267 - val_loss: 1.4035 - val_acc: 0.6016
Epoch 22/150
 - 50s - loss: 1.9094 - acc: 0.7271 - val_loss: 1.4071 - val_acc: 0.5960
Epoch 23/150
 - 50s - loss: 1.9093 - acc: 0.7272 - val_loss: 1.3836 - val_acc: 0.6056
Epoch 24/150
 - 50s - loss: 1.9093 - acc: 0.7270 - val_loss: 1.4518 - val_acc: 0.5694
Epoch 25/150
 - 51s - loss: 1.9092 - acc: 0.7275 - val_loss: 1.4090 - val_acc: 0.5965
Epoch 26/150
 - 52s - loss: 1.9091 - acc: 0.7298 - val_loss: 1.3899 - val_acc: 0.6074
Epoch 27/150
 - 51s - loss: 1.9091 - acc: 0.7285 - val_loss: 1.3932 - val_acc: 0.6083
Epoch 28/150
 - 50s - loss: 1.9090 - acc: 0.7286 - val_loss: 1.3860 - val_acc: 0.6162
Epoch 29/150
 - 51s - loss: 1.9089 - acc: 0.7280 - val_loss: 1.4143 - val_acc: 0.5914
Epoch 30/150
 - 52s - loss: 1.9088 - acc: 0.7260 - val_loss: 1.4076 - val_acc: 0.6071
Epoch 31/150
 - 52s - loss: 1.9088 - acc: 0.7260 - val_loss: 1.3914 - val_acc: 0.6163
Epoch 32/150
 - 52s - loss: 1.9087 - acc: 0.7244 - val_loss: 1.4345 - val_acc: 0.5758
Epoch 33/150
 - 51s - loss: 1.9087 - acc: 0.7247 - val_loss: 1.4457 - val_acc: 0.5640
Epoch 34/150
 - 51s - loss: 1.9086 - acc: 0.7236 - val_loss: 1.5322 - val_acc: 0.5057
Epoch 35/150
 - 51s - loss: 1.9087 - acc: 0.7159 - val_loss: 1.3788 - val_acc: 0.6162
Epoch 36/150
 - 52s - loss: 1.9085 - acc: 0.7164 - val_loss: 1.3956 - val_acc: 0.6013
Epoch 37/150
 - 52s - loss: 1.9085 - acc: 0.7056 - val_loss: 1.4852 - val_acc: 0.5373
Epoch 38/150
 - 52s - loss: 1.9084 - acc: 0.6980 - val_loss: 1.4111 - val_acc: 0.6024
Epoch 39/150
 - 52s - loss: 1.9084 - acc: 0.6943 - val_loss: 1.3986 - val_acc: 0.5949
Epoch 40/150
 - 51s - loss: 1.9083 - acc: 0.6939 - val_loss: 1.4008 - val_acc: 0.6028
Epoch 41/150
 - 52s - loss: 1.9083 - acc: 0.6933 - val_loss: 1.3952 - val_acc: 0.6032
Epoch 42/150
 - 52s - loss: 1.9082 - acc: 0.6928 - val_loss: 1.3841 - val_acc: 0.6105
Epoch 43/150
 - 51s - loss: 1.9082 - acc: 0.6915 - val_loss: 1.4199 - val_acc: 0.5974
Epoch 44/150
 - 51s - loss: 1.9082 - acc: 0.6905 - val_loss: 1.3955 - val_acc: 0.6016
Epoch 45/150
 - 51s - loss: 1.9082 - acc: 0.6892 - val_loss: 1.4181 - val_acc: 0.6025
Epoch 46/150
 - 52s - loss: 1.9081 - acc: 0.6906 - val_loss: 1.4204 - val_acc: 0.6025
Epoch 47/150
 - 52s - loss: 1.9081 - acc: 0.6901 - val_loss: 1.4159 - val_acc: 0.5846
Epoch 48/150
 - 51s - loss: 1.9081 - acc: 0.6892 - val_loss: 1.5082 - val_acc: 0.5254
Epoch 49/150
 - 51s - loss: 1.9081 - acc: 0.6890 - val_loss: 1.3949 - val_acc: 0.6085
Epoch 50/150
 - 53s - loss: 1.9081 - acc: 0.6900 - val_loss: 1.4091 - val_acc: 0.6011
Epoch 51/150
 - 52s - loss: 1.9080 - acc: 0.6896 - val_loss: 1.3855 - val_acc: 0.6163
Epoch 52/150
 - 51s - loss: 1.9079 - acc: 0.6900 - val_loss: 1.3881 - val_acc: 0.6041
Epoch 53/150
 - 52s - loss: 1.9080 - acc: 0.6884 - val_loss: 1.3762 - val_acc: 0.6196
Epoch 54/150
 - 52s - loss: 1.9080 - acc: 0.6890 - val_loss: 1.3935 - val_acc: 0.6059
Epoch 55/150
 - 52s - loss: 1.9079 - acc: 0.6896 - val_loss: 1.3817 - val_acc: 0.6154
Epoch 56/150
 - 51s - loss: 1.9079 - acc: 0.6891 - val_loss: 1.3997 - val_acc: 0.6091
Epoch 57/150
 - 51s - loss: 1.9079 - acc: 0.6895 - val_loss: 1.4346 - val_acc: 0.5866
Epoch 58/150
 - 51s - loss: 1.9080 - acc: 0.6871 - val_loss: 1.3741 - val_acc: 0.6154
Epoch 59/150
 - 51s - loss: 1.9078 - acc: 0.6896 - val_loss: 1.4004 - val_acc: 0.6071
Epoch 60/150
 - 51s - loss: 1.9079 - acc: 0.6901 - val_loss: 1.3835 - val_acc: 0.6110
Epoch 61/150
 - 51s - loss: 1.9078 - acc: 0.6891 - val_loss: 1.3893 - val_acc: 0.6130
Epoch 62/150
 - 51s - loss: 1.9078 - acc: 0.6908 - val_loss: 1.3830 - val_acc: 0.6178
Epoch 63/150
 - 51s - loss: 1.9078 - acc: 0.6898 - val_loss: 1.3844 - val_acc: 0.6127
Epoch 64/150
 - 51s - loss: 1.9078 - acc: 0.6937 - val_loss: 1.4088 - val_acc: 0.6046
Epoch 65/150
 - 53s - loss: 1.9078 - acc: 0.6931 - val_loss: 1.3845 - val_acc: 0.6139
Epoch 66/150
 - 52s - loss: 1.9077 - acc: 0.6944 - val_loss: 1.3866 - val_acc: 0.6161
Epoch 67/150
 - 51s - loss: 1.9077 - acc: 0.6961 - val_loss: 1.3766 - val_acc: 0.6148
Epoch 68/150
 - 51s - loss: 1.9077 - acc: 0.6980 - val_loss: 1.3915 - val_acc: 0.6137
Epoch 69/150
 - 50s - loss: 1.9077 - acc: 0.6980 - val_loss: 1.3815 - val_acc: 0.6190
Epoch 70/150
 - 52s - loss: 1.9077 - acc: 0.6990 - val_loss: 1.3968 - val_acc: 0.6088
Epoch 71/150
 - 51s - loss: 1.9078 - acc: 0.6975 - val_loss: 1.3742 - val_acc: 0.6190
Epoch 72/150
 - 51s - loss: 1.9077 - acc: 0.7004 - val_loss: 1.3874 - val_acc: 0.6134
Epoch 73/150
 - 51s - loss: 1.9076 - acc: 0.6999 - val_loss: 1.3934 - val_acc: 0.6153
Epoch 74/150
 - 52s - loss: 1.9077 - acc: 0.6999 - val_loss: 1.4144 - val_acc: 0.5977
Epoch 75/150
 - 51s - loss: 1.9077 - acc: 0.6995 - val_loss: 1.4026 - val_acc: 0.6080
Epoch 76/150
 - 51s - loss: 1.9076 - acc: 0.7003 - val_loss: 1.4143 - val_acc: 0.5818
Epoch 77/150
 - 51s - loss: 1.9076 - acc: 0.6996 - val_loss: 1.3748 - val_acc: 0.6131
Epoch 78/150
 - 52s - loss: 1.9076 - acc: 0.7003 - val_loss: 1.4304 - val_acc: 0.5851
Epoch 79/150
 - 52s - loss: 1.9075 - acc: 0.7012 - val_loss: 1.4185 - val_acc: 0.5912
Epoch 80/150
 - 52s - loss: 1.9076 - acc: 0.6993 - val_loss: 1.4642 - val_acc: 0.5594
Epoch 81/150
 - 51s - loss: 1.9076 - acc: 0.6992 - val_loss: 1.3756 - val_acc: 0.6107
Epoch 82/150
 - 50s - loss: 1.9076 - acc: 0.6976 - val_loss: 1.3760 - val_acc: 0.6179
Epoch 83/150
 - 50s - loss: 1.9075 - acc: 0.6972 - val_loss: 1.3850 - val_acc: 0.6079
Epoch 84/150
 - 50s - loss: 1.9075 - acc: 0.6988 - val_loss: 1.3862 - val_acc: 0.6076
Epoch 85/150
 - 51s - loss: 1.9075 - acc: 0.6978 - val_loss: 1.3923 - val_acc: 0.6111
Epoch 86/150
 - 51s - loss: 1.9075 - acc: 0.6976 - val_loss: 1.4307 - val_acc: 0.5903
Epoch 87/150
 - 51s - loss: 1.9075 - acc: 0.6984 - val_loss: 1.3893 - val_acc: 0.6123
Epoch 88/150
 - 51s - loss: 1.9075 - acc: 0.6982 - val_loss: 1.4024 - val_acc: 0.6029
Epoch 89/150
 - 50s - loss: 1.9075 - acc: 0.6996 - val_loss: 1.4088 - val_acc: 0.5989
Epoch 90/150
 - 51s - loss: 1.9076 - acc: 0.6961 - val_loss: 1.4542 - val_acc: 0.5593
Epoch 91/150
 - 52s - loss: 1.9076 - acc: 0.6964 - val_loss: 1.4104 - val_acc: 0.5944
Epoch 92/150
 - 51s - loss: 1.9074 - acc: 0.6999 - val_loss: 1.3727 - val_acc: 0.6149
Epoch 93/150
 - 50s - loss: 1.9074 - acc: 0.6978 - val_loss: 1.3963 - val_acc: 0.6061
Epoch 94/150
 - 50s - loss: 1.9074 - acc: 0.6974 - val_loss: 1.3876 - val_acc: 0.6186
Epoch 95/150
 - 50s - loss: 1.9074 - acc: 0.6966 - val_loss: 1.3859 - val_acc: 0.6085
Epoch 96/150
 - 50s - loss: 1.9074 - acc: 0.6971 - val_loss: 1.4160 - val_acc: 0.5948
Epoch 97/150
 - 51s - loss: 1.9074 - acc: 0.6976 - val_loss: 1.3999 - val_acc: 0.6093
Epoch 98/150
 - 51s - loss: 1.9074 - acc: 0.6983 - val_loss: 1.4245 - val_acc: 0.5842
Epoch 99/150
 - 50s - loss: 1.9074 - acc: 0.6975 - val_loss: 1.3752 - val_acc: 0.6152
Epoch 100/150
 - 51s - loss: 1.9074 - acc: 0.6966 - val_loss: 1.5223 - val_acc: 0.5069
Epoch 101/150
 - 52s - loss: 1.9074 - acc: 0.6967 - val_loss: 1.4105 - val_acc: 0.6005
Epoch 102/150
 - 50s - loss: 1.9074 - acc: 0.6969 - val_loss: 1.3761 - val_acc: 0.6191
Epoch 103/150
 - 52s - loss: 1.9074 - acc: 0.6957 - val_loss: 1.3836 - val_acc: 0.6194
Epoch 104/150
 - 51s - loss: 1.9074 - acc: 0.6966 - val_loss: 1.3810 - val_acc: 0.6182
Epoch 105/150
 - 52s - loss: 1.9073 - acc: 0.6976 - val_loss: 1.3783 - val_acc: 0.6176
Epoch 106/150
 - 51s - loss: 1.9073 - acc: 0.6968 - val_loss: 1.3872 - val_acc: 0.6159
Epoch 107/150
 - 51s - loss: 1.9074 - acc: 0.6963 - val_loss: 1.4149 - val_acc: 0.5810
Epoch 108/150
 - 51s - loss: 1.9074 - acc: 0.6966 - val_loss: 1.4069 - val_acc: 0.5836
Epoch 109/150
 - 52s - loss: 1.9073 - acc: 0.6963 - val_loss: 1.5097 - val_acc: 0.5144
Epoch 110/150
 - 51s - loss: 1.9074 - acc: 0.6966 - val_loss: 1.3978 - val_acc: 0.6106
Epoch 111/150
 - 51s - loss: 1.9073 - acc: 0.6965 - val_loss: 1.4149 - val_acc: 0.6016
Epoch 112/150
 - 50s - loss: 1.9073 - acc: 0.6957 - val_loss: 1.3929 - val_acc: 0.5958
Epoch 113/150
 - 50s - loss: 1.9073 - acc: 0.6961 - val_loss: 1.3677 - val_acc: 0.6161
Epoch 114/150
 - 50s - loss: 1.9073 - acc: 0.6952 - val_loss: 1.4322 - val_acc: 0.5861
Epoch 115/150
 - 50s - loss: 1.9073 - acc: 0.6954 - val_loss: 1.3833 - val_acc: 0.6134
Epoch 116/150
 - 50s - loss: 1.9072 - acc: 0.6965 - val_loss: 1.4224 - val_acc: 0.5938
Epoch 117/150
 - 51s - loss: 1.9073 - acc: 0.6947 - val_loss: 1.3940 - val_acc: 0.5951
Epoch 118/150
 - 50s - loss: 1.9073 - acc: 0.6957 - val_loss: 1.4771 - val_acc: 0.5464
Epoch 119/150
 - 51s - loss: 1.9073 - acc: 0.6974 - val_loss: 1.3894 - val_acc: 0.6191
Epoch 120/150
 - 51s - loss: 1.9073 - acc: 0.6957 - val_loss: 1.3900 - val_acc: 0.6164
Epoch 121/150
 - 51s - loss: 1.9072 - acc: 0.6963 - val_loss: 1.3721 - val_acc: 0.6186
Epoch 122/150
 - 51s - loss: 1.9072 - acc: 0.6961 - val_loss: 1.3889 - val_acc: 0.6187
Epoch 123/150
 - 51s - loss: 1.9072 - acc: 0.6956 - val_loss: 1.3792 - val_acc: 0.6189
Epoch 124/150
 - 51s - loss: 1.9072 - acc: 0.6951 - val_loss: 1.3878 - val_acc: 0.6002
Epoch 125/150
 - 51s - loss: 1.9073 - acc: 0.6952 - val_loss: 1.3784 - val_acc: 0.6163
Epoch 126/150
 - 52s - loss: 1.9072 - acc: 0.6956 - val_loss: 1.3680 - val_acc: 0.6172
Epoch 127/150
 - 51s - loss: 1.9073 - acc: 0.6949 - val_loss: 1.3986 - val_acc: 0.6061
Epoch 128/150
 - 52s - loss: 1.9072 - acc: 0.6959 - val_loss: 1.3825 - val_acc: 0.6136
Epoch 129/150
 - 51s - loss: 1.9072 - acc: 0.6959 - val_loss: 1.4622 - val_acc: 0.5792
Epoch 130/150
 - 51s - loss: 1.9072 - acc: 0.6959 - val_loss: 1.3940 - val_acc: 0.6111
Epoch 131/150
 - 51s - loss: 1.9072 - acc: 0.6961 - val_loss: 1.3796 - val_acc: 0.6092
Epoch 132/150
 - 51s - loss: 1.9072 - acc: 0.6950 - val_loss: 1.3811 - val_acc: 0.6197
Epoch 133/150
 - 52s - loss: 1.9072 - acc: 0.6960 - val_loss: 1.3817 - val_acc: 0.6168
Epoch 134/150
 - 51s - loss: 1.9072 - acc: 0.6955 - val_loss: 1.3934 - val_acc: 0.6064
Epoch 135/150
 - 50s - loss: 1.9072 - acc: 0.6956 - val_loss: 1.3797 - val_acc: 0.6176
Epoch 136/150
 - 52s - loss: 1.9072 - acc: 0.6962 - val_loss: 1.3797 - val_acc: 0.6206
Epoch 137/150
 - 51s - loss: 1.9072 - acc: 0.6961 - val_loss: 1.4441 - val_acc: 0.5790
Epoch 138/150
 - 52s - loss: 1.9072 - acc: 0.6944 - val_loss: 1.3787 - val_acc: 0.6095
Epoch 139/150
 - 52s - loss: 1.9071 - acc: 0.6972 - val_loss: 1.3813 - val_acc: 0.6151
Epoch 140/150
 - 51s - loss: 1.9072 - acc: 0.6947 - val_loss: 1.4168 - val_acc: 0.5891
Epoch 141/150
 - 51s - loss: 1.9072 - acc: 0.6937 - val_loss: 1.3817 - val_acc: 0.6051
Epoch 142/150
 - 53s - loss: 1.9071 - acc: 0.6954 - val_loss: 1.3833 - val_acc: 0.6115
Epoch 143/150
 - 52s - loss: 1.9071 - acc: 0.6960 - val_loss: 1.4433 - val_acc: 0.5808
Epoch 144/150
 - 51s - loss: 1.9071 - acc: 0.6959 - val_loss: 1.4116 - val_acc: 0.5912
Epoch 145/150
 - 51s - loss: 1.9071 - acc: 0.6959 - val_loss: 1.3777 - val_acc: 0.6202
Epoch 146/150
 - 50s - loss: 1.9071 - acc: 0.6943 - val_loss: 1.4093 - val_acc: 0.6147
Epoch 147/150
 - 51s - loss: 1.9071 - acc: 0.6955 - val_loss: 1.3911 - val_acc: 0.6140
Epoch 148/150
 - 51s - loss: 1.9071 - acc: 0.6951 - val_loss: 1.4386 - val_acc: 0.5775
Epoch 149/150
 - 51s - loss: 1.9071 - acc: 0.6950 - val_loss: 1.4002 - val_acc: 0.6003
Epoch 150/150
 - 52s - loss: 1.9071 - acc: 0.6947 - val_loss: 1.4129 - val_acc: 0.6084
('small_ResNetLikeModel:', [1.3677304016113281, 0.6205833333333333])

Process finished with exit code 0
-------------------------------------------------------------------------------------------------------the quantized NSM 
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
Input (InputLayer)              (None, 128, 2)       0                                            
__________________________________________________________________________________________________
res_stack1_a (Conv1D)           (None, 128, 16)      48          Input[0][0]                      
__________________________________________________________________________________________________
bn_stack1_a (BatchNormalization (None, 128, 16)      64          res_stack1_a[0][0]               
__________________________________________________________________________________________________
res_stack1_blockb_u1 (Conv1D)   (None, 128, 16)      2064        bn_stack1_a[0][0]                
__________________________________________________________________________________________________
bn_stack1_blockb_u1 (BatchNorma (None, 128, 16)      64          res_stack1_blockb_u1[0][0]       
__________________________________________________________________________________________________
activation_61 (Activation)      (None, 128, 16)      0           bn_stack1_blockb_u1[0][0]        
__________________________________________________________________________________________________
res_stack1_blockb_u2 (Conv1D)   (None, 128, 16)      272         activation_61[0][0]              
__________________________________________________________________________________________________
bn_stack1_blockb_u2 (BatchNorma (None, 128, 16)      64          res_stack1_blockb_u2[0][0]       
__________________________________________________________________________________________________
add_29 (Add)                    (None, 128, 16)      0           bn_stack1_blockb_u2[0][0]        
                                                                 bn_stack1_a[0][0]                
__________________________________________________________________________________________________
activation_62 (Activation)      (None, 128, 16)      0           add_29[0][0]                     
__________________________________________________________________________________________________
res_stack1_blockc_u1 (Conv1D)   (None, 128, 16)      2064        activation_62[0][0]              
__________________________________________________________________________________________________
bn_stack1_blockc_u1 (BatchNorma (None, 128, 16)      64          res_stack1_blockc_u1[0][0]       
__________________________________________________________________________________________________
activation_63 (Activation)      (None, 128, 16)      0           bn_stack1_blockc_u1[0][0]        
__________________________________________________________________________________________________
res_stack1_blockc_u2 (Conv1D)   (None, 128, 16)      272         activation_63[0][0]              
__________________________________________________________________________________________________
bn_stack1_blockc_u2 (BatchNorma (None, 128, 16)      64          res_stack1_blockc_u2[0][0]       
__________________________________________________________________________________________________
add_30 (Add)                    (None, 128, 16)      0           bn_stack1_blockc_u2[0][0]        
                                                                 activation_62[0][0]              
__________________________________________________________________________________________________
activation_64 (Activation)      (None, 128, 16)      0           add_30[0][0]                     
__________________________________________________________________________________________________
mp_stack1 (MaxPooling1D)        (None, 128, 16)      0           activation_64[0][0]              
__________________________________________________________________________________________________
res_stack2_a (Conv1D)           (None, 128, 32)      544         mp_stack1[0][0]                  
__________________________________________________________________________________________________
bn_stack2_a (BatchNormalization (None, 128, 32)      128         res_stack2_a[0][0]               
__________________________________________________________________________________________________
res_stack2_blockb_u1 (Conv1D)   (None, 128, 32)      8224        bn_stack2_a[0][0]                
__________________________________________________________________________________________________
bn_stack2_blockb_u1 (BatchNorma (None, 128, 32)      128         res_stack2_blockb_u1[0][0]       
__________________________________________________________________________________________________
activation_65 (Activation)      (None, 128, 32)      0           bn_stack2_blockb_u1[0][0]        
__________________________________________________________________________________________________
res_stack2_blockb_u2 (Conv1D)   (None, 128, 32)      1056        activation_65[0][0]              
__________________________________________________________________________________________________
bn_stack2_blockb_u2 (BatchNorma (None, 128, 32)      128         res_stack2_blockb_u2[0][0]       
__________________________________________________________________________________________________
add_31 (Add)                    (None, 128, 32)      0           bn_stack2_blockb_u2[0][0]        
                                                                 bn_stack2_a[0][0]                
__________________________________________________________________________________________________
activation_66 (Activation)      (None, 128, 32)      0           add_31[0][0]                     
__________________________________________________________________________________________________
res_stack2_blockc_u1 (Conv1D)   (None, 128, 32)      8224        activation_66[0][0]              
__________________________________________________________________________________________________
bn_stack2_blockc_u1 (BatchNorma (None, 128, 32)      128         res_stack2_blockc_u1[0][0]       
__________________________________________________________________________________________________
activation_67 (Activation)      (None, 128, 32)      0           bn_stack2_blockc_u1[0][0]        
__________________________________________________________________________________________________
res_stack2_blockc_u2 (Conv1D)   (None, 128, 32)      1056        activation_67[0][0]              
__________________________________________________________________________________________________
bn_stack2_blockc_u2 (BatchNorma (None, 128, 32)      128         res_stack2_blockc_u2[0][0]       
__________________________________________________________________________________________________
add_32 (Add)                    (None, 128, 32)      0           bn_stack2_blockc_u2[0][0]        
                                                                 activation_66[0][0]              
__________________________________________________________________________________________________
activation_68 (Activation)      (None, 128, 32)      0           add_32[0][0]                     
__________________________________________________________________________________________________
mp_stack2 (MaxPooling1D)        (None, 128, 32)      0           activation_68[0][0]              
__________________________________________________________________________________________________
pooling_size (MaxPooling1D)     (None, 64, 32)       0           mp_stack2[0][0]                  
__________________________________________________________________________________________________
flatten_3 (Flatten)             (None, 2048)         0           pooling_size[0][0]               
__________________________________________________________________________________________________
small_dense3 (Dense)            (None, 128)          262272      flatten_3[0][0]                  
__________________________________________________________________________________________________
small_dense4 (Dense)            (None, 10)           1290        small_dense3[0][0]               
__________________________________________________________________________________________________
T (Lambda)                      (None, 10)           0           small_dense4[0][0]               
__________________________________________________________________________________________________
activation_69 (Activation)      (None, 10)           0           T[0][0]                          
==================================================================================================
Total params: 288,346
Trainable params: 287,866
Non-trainable params: 480
__________________________________________________________________________________________________

   384/240000 [..............................] - ETA: 11:08
  2304/240000 [..............................] - ETA: 1:56 
  4224/240000 [..............................] - ETA: 1:06
  6144/240000 [..............................] - ETA: 47s 
  8064/240000 [>.............................] - ETA: 37s
  9984/240000 [>.............................] - ETA: 31s
 11904/240000 [>.............................] - ETA: 27s
 13824/240000 [>.............................] - ETA: 24s
 15744/240000 [>.............................] - ETA: 21s
 17664/240000 [=>............................] - ETA: 19s
 19584/240000 [=>............................] - ETA: 18s
 21504/240000 [=>............................] - ETA: 17s
 23424/240000 [=>............................] - ETA: 16s
 25344/240000 [==>...........................] - ETA: 15s
 27264/240000 [==>...........................] - ETA: 14s
 29184/240000 [==>...........................] - ETA: 13s
 31104/240000 [==>...........................] - ETA: 13s
 33024/240000 [===>..........................] - ETA: 12s
 34944/240000 [===>..........................] - ETA: 12s
 36864/240000 [===>..........................] - ETA: 11s
 38784/240000 [===>..........................] - ETA: 11s
 40704/240000 [====>.........................] - ETA: 11s
 42624/240000 [====>.........................] - ETA: 10s
 44544/240000 [====>.........................] - ETA: 10s
 46464/240000 [====>.........................] - ETA: 10s
 48384/240000 [=====>........................] - ETA: 9s 
 50304/240000 [=====>........................] - ETA: 9s
 52224/240000 [=====>........................] - ETA: 9s
 54144/240000 [=====>........................] - ETA: 9s
 56064/240000 [======>.......................] - ETA: 8s
 57984/240000 [======>.......................] - ETA: 8s
 59904/240000 [======>.......................] - ETA: 8s
 61824/240000 [======>.......................] - ETA: 8s
 63744/240000 [======>.......................] - ETA: 8s
 65664/240000 [=======>......................] - ETA: 7s
 67584/240000 [=======>......................] - ETA: 7s
 69504/240000 [=======>......................] - ETA: 7s
 71424/240000 [=======>......................] - ETA: 7s
 72960/240000 [========>.....................] - ETA: 7s
 74880/240000 [========>.....................] - ETA: 7s
 77184/240000 [========>.....................] - ETA: 7s
 79104/240000 [========>.....................] - ETA: 6s
 81024/240000 [=========>....................] - ETA: 6s
 82944/240000 [=========>....................] - ETA: 6s
 84864/240000 [=========>....................] - ETA: 6s
 86784/240000 [=========>....................] - ETA: 6s
 88704/240000 [==========>...................] - ETA: 6s
 90624/240000 [==========>...................] - ETA: 6s
 92544/240000 [==========>...................] - ETA: 6s
 94464/240000 [==========>...................] - ETA: 5s
 96384/240000 [===========>..................] - ETA: 5s
 98304/240000 [===========>..................] - ETA: 5s
100224/240000 [===========>..................] - ETA: 5s
102144/240000 [===========>..................] - ETA: 5s
104064/240000 [============>.................] - ETA: 5s
105984/240000 [============>.................] - ETA: 5s
107904/240000 [============>.................] - ETA: 5s
109824/240000 [============>.................] - ETA: 5s
111744/240000 [============>.................] - ETA: 4s
113664/240000 [=============>................] - ETA: 4s
115584/240000 [=============>................] - ETA: 4s
117504/240000 [=============>................] - ETA: 4s
119424/240000 [=============>................] - ETA: 4s
121344/240000 [==============>...............] - ETA: 4s
123264/240000 [==============>...............] - ETA: 4s
125184/240000 [==============>...............] - ETA: 4s
127104/240000 [==============>...............] - ETA: 4s
129024/240000 [===============>..............] - ETA: 4s
130944/240000 [===============>..............] - ETA: 4s
132864/240000 [===============>..............] - ETA: 4s
134784/240000 [===============>..............] - ETA: 3s
136704/240000 [================>.............] - ETA: 3s
138624/240000 [================>.............] - ETA: 3s
140544/240000 [================>.............] - ETA: 3s
142464/240000 [================>.............] - ETA: 3s
144384/240000 [=================>............] - ETA: 3s
146304/240000 [=================>............] - ETA: 3s
148224/240000 [=================>............] - ETA: 3s
150144/240000 [=================>............] - ETA: 3s
152064/240000 [==================>...........] - ETA: 3s
153984/240000 [==================>...........] - ETA: 3s
155904/240000 [==================>...........] - ETA: 3s
157824/240000 [==================>...........] - ETA: 2s
159744/240000 [==================>...........] - ETA: 2s
161664/240000 [===================>..........] - ETA: 2s
163584/240000 [===================>..........] - ETA: 2s
165504/240000 [===================>..........] - ETA: 2s
167424/240000 [===================>..........] - ETA: 2s
169344/240000 [====================>.........] - ETA: 2s
171264/240000 [====================>.........] - ETA: 2s
173184/240000 [====================>.........] - ETA: 2s
175104/240000 [====================>.........] - ETA: 2s
177024/240000 [=====================>........] - ETA: 2s
178944/240000 [=====================>........] - ETA: 2s
180480/240000 [=====================>........] - ETA: 2s
182400/240000 [=====================>........] - ETA: 2s
184320/240000 [======================>.......] - ETA: 1s
186240/240000 [======================>.......] - ETA: 1s
188160/240000 [======================>.......] - ETA: 1s
190080/240000 [======================>.......] - ETA: 1s
192000/240000 [=======================>......] - ETA: 1s
193920/240000 [=======================>......] - ETA: 1s
195840/240000 [=======================>......] - ETA: 1s
197760/240000 [=======================>......] - ETA: 1s
199680/240000 [=======================>......] - ETA: 1s
201600/240000 [========================>.....] - ETA: 1s
203520/240000 [========================>.....] - ETA: 1s
205440/240000 [========================>.....] - ETA: 1s
207360/240000 [========================>.....] - ETA: 1s
209280/240000 [=========================>....] - ETA: 1s
211200/240000 [=========================>....] - ETA: 0s
213120/240000 [=========================>....] - ETA: 0s
215040/240000 [=========================>....] - ETA: 0s
216960/240000 [==========================>...] - ETA: 0s
218880/240000 [==========================>...] - ETA: 0s
220800/240000 [==========================>...] - ETA: 0s
222720/240000 [==========================>...] - ETA: 0s
224640/240000 [===========================>..] - ETA: 0s
226560/240000 [===========================>..] - ETA: 0s
228480/240000 [===========================>..] - ETA: 0s
230400/240000 [===========================>..] - ETA: 0s
232320/240000 [============================>.] - ETA: 0s
234240/240000 [============================>.] - ETA: 0s
236160/240000 [============================>.] - ETA: 0s
238080/240000 [============================>.] - ETA: 0s
240000/240000 [==============================] - 8s 34us/step
('quan_16_small_ResNetLikeModel:', [0.97554023437500004, 0.6211250000000000])

